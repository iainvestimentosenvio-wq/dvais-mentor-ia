/**
 * AI Models Management
 * 
 * Gerenciamento de modelos de IA (TensorFlow.js)
 * Cache inteligente e lazy loading de modelos
 */

'use client'

// Tipos para modelos (ser√£o carregados dinamicamente quando necess√°rio)
type TFModel = unknown

/**
 * Tipo de modelo
 */
export type ModelType = 'classification' | 'regression' | 'detection' | 'custom'

/**
 * Informa√ß√µes do modelo
 */
export interface ModelInfo {
  name: string
  type: ModelType
  url: string
  loaded: boolean
  size?: number
}

/**
 * Cache de modelos carregados
 */
const modelCache = new Map<string, TFModel>()

/**
 * Registro de modelos dispon√≠veis
 */
const modelRegistry: Map<string, ModelInfo> = new Map()

/**
 * Registra um modelo
 */
export function registerModel(name: string, info: Omit<ModelInfo, 'loaded'>): void {
  modelRegistry.set(name, {
    ...info,
    loaded: false,
  })
  if (process.env.NODE_ENV !== 'production') {
    console.log(`üìã Modelo registrado: ${name}`)
  }
}

/**
 * Carrega um modelo (com cache)
 * Nota: Requer @tensorflow/tfjs completo (n√£o apenas core)
 */
export async function loadModel(name: string): Promise<TFModel> {
  // Verificar se j√° est√° em cache
  if (modelCache.has(name)) {
    if (process.env.NODE_ENV !== 'production') {
      console.log(`‚úÖ Modelo em cache: ${name}`)
    }
    return modelCache.get(name)!
  }
  
  // Verificar se est√° registrado
  const modelInfo = modelRegistry.get(name)
  if (!modelInfo) {
    throw new Error(`Modelo n√£o registrado: ${name}`)
  }
  
  try {
    if (process.env.NODE_ENV !== 'production') {
      console.log(`‚è≥ Carregando modelo: ${name} (${modelInfo.url})`)
    }
    const startTime = performance.now()
    
    // Carregar modelo (importa√ß√£o din√¢mica do TensorFlow.js completo)
    const tf = await import('@tensorflow/tfjs')
    const model = await tf.loadLayersModel(modelInfo.url)
    
    const endTime = performance.now()
    const loadTime = endTime - startTime
    
    // Adicionar ao cache
    modelCache.set(name, model)
    modelInfo.loaded = true
    
    if (process.env.NODE_ENV !== 'production') {
      console.log(`‚úÖ Modelo carregado: ${name} (${loadTime.toFixed(2)}ms)`)
    }
    
    return model
  } catch (error) {
    console.error(`‚ùå Erro ao carregar modelo ${name}:`, error)
    throw error
  }
}

/**
 * Descarrega um modelo (libera mem√≥ria)
 */
export async function unloadModel(name: string): Promise<void> {
  const model = modelCache.get(name)
  if (model) {
    // Verificar se o modelo tem m√©todo dispose (TensorFlow.js)
    if (model && typeof model === 'object' && 'dispose' in model && typeof (model as { dispose: () => void }).dispose === 'function') {
      (model as { dispose: () => void }).dispose()
    }
    modelCache.delete(name)
    
    const modelInfo = modelRegistry.get(name)
    if (modelInfo) {
      modelInfo.loaded = false
    }
    
    if (process.env.NODE_ENV !== 'production') {
      console.log(`üóëÔ∏è Modelo descarregado: ${name}`)
    }
  }
}

/**
 * Lista modelos dispon√≠veis
 */
export function listModels(): ModelInfo[] {
  return Array.from(modelRegistry.values())
}

/**
 * Verifica se um modelo est√° carregado
 */
export function isModelLoaded(name: string): boolean {
  return modelCache.has(name)
}

/**
 * Limpa todos os modelos (libera mem√≥ria)
 */
export async function clearModelCache(): Promise<void> {
  // Usar Array.from para evitar problemas com downlevelIteration
  const entries = Array.from(modelCache.entries())
  for (const [name, model] of entries) {
    // Verificar se o modelo tem m√©todo dispose (TensorFlow.js)
    if (model && typeof model === 'object' && 'dispose' in model && typeof (model as { dispose: () => void }).dispose === 'function') {
      (model as { dispose: () => void }).dispose()
    }
    if (process.env.NODE_ENV !== 'production') {
      console.log(`üóëÔ∏è Modelo descarregado: ${name}`)
    }
  }
  
  modelCache.clear()
  
  // Usar Array.from para evitar problemas com downlevelIteration
  const modelInfos = Array.from(modelRegistry.values())
  for (const modelInfo of modelInfos) {
    modelInfo.loaded = false
  }
  
  if (process.env.NODE_ENV !== 'production') {
    console.log('‚úÖ Cache de modelos limpo')
  }
}

/**
 * Retorna informa√ß√µes de mem√≥ria usada pelos modelos
 * Nota: Requer TensorFlow.js carregado
 */
export async function getMemoryInfo(): Promise<{
  numTensors: number
  numBytes: number
  numBytesFormatted: string
}> {
  const tf = await import('@tensorflow/tfjs')
  const memoryInfo = tf.memory()
  
  return {
    numTensors: memoryInfo.numTensors,
    numBytes: memoryInfo.numBytes,
    numBytesFormatted: formatBytes(memoryInfo.numBytes),
  }
}

/**
 * Formata bytes para formato leg√≠vel
 */
function formatBytes(bytes: number): string {
  if (bytes === 0) return '0 Bytes'
  
  const k = 1024
  const sizes = ['Bytes', 'KB', 'MB', 'GB']
  const i = Math.floor(Math.log(bytes) / Math.log(k))
  
  return Math.round((bytes / Math.pow(k, i)) * 100) / 100 + ' ' + sizes[i]
}

// Exemplo de modelo pr√©-registrado (placeholder)
// Em produ√ß√£o, registre seus modelos reais aqui
if (typeof window !== 'undefined') {
  // registerModel('example-model', {
  //   name: 'Example Model',
  //   type: 'classification',
  //   url: '/models/example-model/model.json',
  // })
}

